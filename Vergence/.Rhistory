df$enhancenum<- stimes[[4]]
df$enhance.dur<- stimes[[3]]
return(df)
# return(stimes[[1]]:stimes[[2]])
}
stimes %>%
mutate(dur=event.offset-event.onset,
s=row_number())->
stimes
x<- do.call('rbind',apply(stimes,1,jsac))
v<- data.frame(v=v)
v<- mutate(v, time=row_number())
xx<- left_join(v,x,by='time')
xx %>%
group_by(enhancenum) %>%
summarize(max.vel=max(abs(v))) %>%
filter(max.vel>threshold1)->
xm
xx %>%
filter(enhancenum %in% unique(xm$enhancenum)) %>%
dplyr::select(time,v,enhancenum) ->
g
}
#load all the .csv files in the data folder, then add a column naming the neuron,
#using the file name as the default name, then put them all together in one long data frame
t<- loadnewcsv(path="C:/Users/setup/Desktop/SOA/",referencefile=NULL)
t %>%
group_by(neuron) %>%
mutate(verg.velocity=lev-rev,
sdf = spikedensity(rasters,sd=10),
time=row_number())->
t
# t<- rbind(o,t)
# o<-NULL
t %>%
group_by(neuron) %>%
mutate(s=markSaccades(conj.velocity,buffer=10,threshold=10),
isfixation=s<0) %>%
filter(isfixation) %>% #This removes all saccades from the dataframe
group_by(neuron,s) %>%
mutate(meanfr=mean(sdf),
maxfr=max(sdf),
R.Hor=mean(rep),
R.Ver=mean(repV),
L.Hor=mean(lep),
L.Ver=mean(lepV),
# exatrope= monkey=='Pilchuck',
mean.Verg.Angle=mean(verg.angle),
mean.Verg.Vel=mean(verg.velocity),
# mean.Verg.Angle=replace(mean.Verg.Angle, mean.Verg.Angle<0 & ~exatrope , NA),
# mean.Verg.Angle=replace(mean.Verg.Angle, mean.Verg.Angle>0 & exatrope,NA ),
# mean.Verg.Angle=replace(mean.Verg.Angle,abs(mean.Verg.Angle)>50, NA),
max.Verg.Vel = max(verg.velocity),
max.Verg.Ang = max(verg.angle),
nspikes=sum(rasters),
dur=n(),
mean.Spikerate=sum(rasters)/dur*1000,
R.H.Amp=rep[1]-rep[length(rep)],
L.H.Amp=lep[1]-lep[length(lep)],
R.V.Amp=repV[1]-repV[length(repV)],
L.V.Amp=lepV[1]-lepV[length(lepV)],
maxamp=max(abs(R.H.Amp),abs(R.V.Amp),abs(L.H.Amp),abs(L.V.Amp)))->
m
t %>%
group_by(neuron) %>%
mutate(time=row_number()) %>%
do(markEnhancement(v=.$verg.velocity,threshold2=12,threshold1=50))->
zz
zz<- dplyr::select(zz,time,enhancenum)
t<- left_join(t,zz,by=c('neuron','time'))
t<-ungroup(t)
#determine whether it's convergence or divergence
t %>%
mutate(verg.enhance=!is.na(enhancenum),
enhance.type='none',
verg.direction=verg.velocity>0)->
t
i<- t$verg.enhance & !t$verg.direction
t$enhance.type[i]<- 'divergence'
i<- t$verg.enhance & t$verg.direction
t$enhance.type[i]<- 'convergence'
t$enhance.type<- as.factor(t$enhance.type)
t$verg.direction<- NULL
# t %>% group_by(neuron) %>%
#   do(dynamiclead(p=.,formula='verg.angle+verg.velocity:enhance.type',seq(5,200,by=5))) ->
#   t
# saveRDS(t,'enhancemarked2.RDS')
head(t)
t$enhance.type[1:4]
sum(t$enhance.type=='convergence')
#load all the .csv files in the data folder, then add a column naming the neuron,
#using the file name as the default name, then put them all together in one long data frame
t<- loadnewcsv(path="C:/Users/setup/Desktop/SOA/",referencefile=NULL)
t %>%
group_by(neuron) %>%
mutate(verg.velocity=lev-rev,
sdf = spikedensity(rasters,sd=10),
time=row_number())->
t
# t<- rbind(o,t)
# o<-NULL
t %>%
group_by(neuron) %>%
mutate(time=row_number()) %>%
do(markEnhancement(v=.$verg.velocity,threshold2=12,threshold1=50))->
zz
head(zz)
zz<- dplyr::select(zz,time,enhancenum)
head(zz)
t %>%
group_by(neuron) %>%
mutate(time=row_number()) %>%
do(markEnhancement(v=.$verg.velocity,threshold2=12,threshold1=50))->
zz
t %>%
group_by(neuron) %>%
mutate(time=row_number()) %>%
do(markEnhancement(v=.$verg.velocity,threshold2=12,threshold1=50))->
zz
zz<- dplyr::select(zz,-v)
tx<- left_join(t,zz,by=c('neuron','time'))
head(tx)
head(zz)
head(t)
t %>%
group_by(neuron) %>%
mutate(time=row_number()) %>%
do(markEnhancement(v=.$verg.velocity,threshold2=12,threshold1=50))->
zz
zz<- dplyr::select(zz,-v)
tx<- left_join(t,zz,by=c('neuron','time'))
# t<-ungroup(tx)
head(tx)
t<- ungroup(tx)
t<- ungroup(tx)
head(t)
class(t)
tx %>%
mutate(verg.enhance=!is.na(enhancenum),
enhance.type='none',
verg.direction=verg.velocity>0)->
tx
head(tx)
tx$enhance.type
i<- t$verg.enhance & !t$verg.direction
t<-tx
i<- t$verg.enhance & !t$verg.direction
t$enhance.type[i]<- 'divergence'
i<- t$verg.enhance & t$verg.direction
t$enhance.type[i]<- 'convergence'
t$enhance.type<- as.factor(t$enhance.type)
t$verg.direction<- NULL
# t<- readRDS('enhancemarked.RDS')
# t<- readRDS('enhancemarked-NRTP.RDS')
# t<- readRDS('enhancemarked2.RDS')
# t %>% group_by(neuron) %>%
#   do(dynamiclead(p=.,formula='verg.angle',seq(5,60,by=5))) ->
#   t
t %>%
group_by(neuron) %>%
mutate(sdf20= lag(sdf,20)) ->
t
t %>% group_by(neuron) %>%
do(joinsaccades(.,buffer=20,threshold=20))->
t
t %>%
group_by(neuron) %>%
# mutate(bin.velocity=cut(verg.velocity,c(seq(-200,200,by=20)))) %>%
# filter(!is.na(sacnum)) %>%
group_by(neuron, sacnum) %>%
mutate(conj.h=(lep+rep)/2,
conj.v=(lepV+repV)/2,
conj.h.amp=last(conj.h)-first(conj.h),
conj.v.amp=last(conj.v)-first(conj.v),
conj.angle=atan2(conj.v,conj.h)*180/pi,
peak.verg.velocity=maxabs(verg.velocity),
r.amp=sqrt(conj.h^2+conj.v^2),
verg.amp=last(verg.angle)-first(verg.angle),
saccade.type='conj',
saccade.type=replace(saccade.type,verg.amp< -2 & r.amp>2,'diverging'),
saccade.type=replace(saccade.type,verg.amp> 2& r.amp>2,'converging'),
saccade.type=replace(saccade.type,is.na(sacnum) | r.amp<2,'no.saccade')) ->
t
t %>% group_by(neuron) %>%
filter(saccade.type != 'conj') %>%
# do(dynamiclead(p=.,formula='verg.angle+verg.velocity',seq(5,200,by=5))) %>%
# do(tidy(lm('sdflag~verg.angle+verg.velocity:enhance.type',data=.))) %>%
#below: test of using standard delay
do(tidy(lm('sdf20~verg.angle+verg.velocity:enhance.type',data=.))) %>%
mutate(term=replace(term,term=='verg.velocity:enhance.typenone','Slow.Vergence'),
term=replace(term,term=='verg.velocity:enhance.typeconvergence','Convergence'),
term=replace(term,term=='verg.velocity:enhance.typedivergence','Divergence'))->
ttest
?tidy
??tidy
t %>% group_by(neuron) %>%
filter(saccade.type != 'conj') %>%
# do(dynamiclead(p=.,formula='verg.angle+verg.velocity',seq(5,200,by=5))) %>%
# do(tidy(lm('sdflag~verg.angle+verg.velocity:enhance.type',data=.))) %>%
#below: test of using standard delay
do(tidyr::tidy(lm('sdf20~verg.angle+verg.velocity:enhance.type',data=.))) %>%
mutate(term=replace(term,term=='verg.velocity:enhance.typenone','Slow.Vergence'),
term=replace(term,term=='verg.velocity:enhance.typeconvergence','Convergence'),
term=replace(term,term=='verg.velocity:enhance.typedivergence','Divergence'))->
ttest
?broom::tidy
library(broom)
install.packages('broom')
library(broom)
?tidy
t %>% group_by(neuron) %>%
filter(saccade.type != 'conj') %>%
# do(dynamiclead(p=.,formula='verg.angle+verg.velocity',seq(5,200,by=5))) %>%
# do(tidy(lm('sdflag~verg.angle+verg.velocity:enhance.type',data=.))) %>%
#below: test of using standard delay
do(tidyr::tidy(lm('sdf20~verg.angle+verg.velocity:enhance.type',data=.))) %>%
mutate(term=replace(term,term=='verg.velocity:enhance.typenone','Slow.Vergence'),
term=replace(term,term=='verg.velocity:enhance.typeconvergence','Convergence'),
term=replace(term,term=='verg.velocity:enhance.typedivergence','Divergence'))->
ttest
t %>% group_by(neuron) %>%
filter(saccade.type != 'conj') %>%
# do(dynamiclead(p=.,formula='verg.angle+verg.velocity',seq(5,200,by=5))) %>%
# do(tidy(lm('sdflag~verg.angle+verg.velocity:enhance.type',data=.))) %>%
#below: test of using standard delay
do(tidy(lm('sdf20~verg.angle+verg.velocity:enhance.type',data=.))) %>%
mutate(term=replace(term,term=='verg.velocity:enhance.typenone','Slow.Vergence'),
term=replace(term,term=='verg.velocity:enhance.typeconvergence','Convergence'),
term=replace(term,term=='verg.velocity:enhance.typedivergence','Divergence'))->
ttest
ttest %>%
dplyr::select(term,estimate) %>%
group_by(term) %>%
summarize(e=mean(estimate),std=sd(estimate))->
m
kable(m)
t %>%
group_by(neuron) %>%
summarize(dynamiclead=first(dynamiclead)) ->
s
ttest %>%
dplyr::select(-statistic,-std.error,-p.value) %>%
spread(term,estimate) ->
tp
kable(tp,digits=c(0,2,2,2,2,2))
5+5
library(ggplot2)
library(dplyr)
library(knitr)
library(tidyr)
library(broom)
# library(grid)
library(relaimpo)
library(leaps)
#library(data.table)
library(stringr)
# library(checkpoint)
# checkpoint("2017-01-20")
spikedensity<-function (rasters,sd=100) {
gsize<- sd*10
g<-dnorm(-gsize:gsize,mean=0,sd=sd)
sdf<-convolve(rasters,g,type="open")
sdf<-sdf[gsize:(length(sdf)-(gsize+1))]*1000
sdf
}
dynamiclead<-function(p,lags=seq(10,300,by=10)) {
rsq<-NULL
for (i in 1:length(lags)) {
if (lags[i] > 0){
p$sdflag<-dplyr::lag(p$sdf,lags[i])
}
else{
p$sdflag<-dplyr::lead(p$sdf,lags[i]*-1)
}
rsq[i]<- summary(lm(sdflag~rep+lep+repV+lepV,data=p))$r.squared
}
#return(rsq)
return(lags[rsq==max(rsq)])
}
findSaccades<-function(ev,threshold=40){
mindur<-50
i<-which(abs(ev)>threshold) #find all the times when speed > threshold
sacoff<-which(diff(i)>mindur) #minimum duration of an accepted saccade
sacon<-c(1,sacoff+1) #first saccade
sacoff<-c(sacoff,length(i)) #end of last saccade
saccade.onset<-i[sacon] #get actual times
saccade.offset<-i[sacoff]
return(data.frame(saccade.onset,saccade.offset))
}
markSaccades<-function(ev,buffer=15,threshold=40){
#this function finds and marks saccades given a velocity input
stimes<-findSaccades(ev,threshold)
#remove saccades without enough data at the end of the file, based on buffer size
toolong<- stimes$saccade.offset> length(ev)-buffer
tooshort<- stimes$saccade.onset<buffer+1
stimes<- filter(stimes, !tooshort, !toolong)
nsaccades=nrow(stimes)
stimes$saccade.onset=stimes$saccade.onset-buffer
stimes$saccade.offset=stimes$saccade.offset+buffer
s<-1:length(ev)*0
for (k in 1:nsaccades){
s[stimes$saccade.onset[k]:stimes$saccade.offset[k]]<- k
if(k>1){
s[stimes$saccade.offset[k-1]:stimes$saccade.onset[k]]<-(k*-1)
}
}
s[1:stimes$saccade.onset[1]]<- -1
s[stimes$saccade.offset[nrow(stimes)]:length(s)]<- (nrow(stimes)*-1)-1
return(s)
}
parabolicdiff <- function(pos,n=7){
q <- sum(2*((1:n)^2))
convoutput<- convolve(pos,c(-n:-1, 1:n),type="open")
convoutput<- convoutput[(n*2):(length(pos)-((n*2)+1))]
vels<- c(array(convoutput[1],dim=n*2),convoutput,array(convoutput[length(convoutput)],dim=n*2))
vels <- vels/q*1000
}
maxabs<- function(x){
m1<-max(x,na.rm=T)
m2<-min(x,na.rm=T)
if (abs(m1)>abs(m2)) {
return(m1)
} else{
return(m2)
}
}
loadnewcsv<- function(referencefile=NULL,path="C:/Users/setup/Desktop/SOA/"){
require(stringr)
require(dplyr)
#This function loads .csv files in a particular folder. They must have the same columns for rbind
#Saves time by only reading the csv when necessary
#get names of all files in path
files <- list.files(path=path,pattern='*.csv')
#extract neuron name eg. Bee-01
names<-sapply(files, str_match,"^[a-zA-Z]+-[0-9]+",USE.NAMES=FALSE)
# check for new cells
if (!is.null(referencefile)){
files<-files[!names %in% referencefile$neuron] #comparison
}
nfiles<-length(files)
if (nfiles>0){
message(c('New Files: ',files))
loadedfiles <- lapply(paste(path,files,sep=''),read.csv)
t<-data.frame()
# t.old<-NULL
for (i in 1:nfiles) {
f<- files[i]
message(paste('Loading:',f))
temp=loadedfiles[[i]]
names<-str_match(f,"(^[a-zA-Z]+)-([0-9]+)")
temp$neuron<-names[1]
temp$monkey<-names[2]
temp$cellnum<-as.numeric(names[3])
# temp$sdf<-spikedensity(temp$rasters,sd=10)
# leadtime<-dynamiclead(temp)
message(paste('ms of data:',nrow(temp)))
if (mean(temp$lep-temp$rep)<0){ #either an exotrope or old data with switched left and right eyes
temp<-rename(temp,lep=rep,rep=lep,lepV=repV, repV=lepV,rev=lev,lev=rev,revV=levV,levV=revV)
message(paste('Swapping L/R in:',names[1]))
}
mutate(temp,
# sdflag=lag(sdf,leadtime),
conj.velocity=sqrt((rev^2+lev^2)/2)+sqrt((revV^2+levV^2)/2),
# s=markSaccades(conj.velocity,buffer=10,threshold=10),
# slong=markSaccades(conj.velocity,buffer=longbuffer,threshold=10),
time=row_number(),
# verg.velocity=parabolicdiff(verg.angle,7),
verg.angle=lep-rep)->
temp
t <-rbind(t,temp)
}
t<- dplyr::select(t, -thp,-tvp,-time)
}else{
message('********NO NEW CELLS********')
t<-NULL
}
return(t)
}
bootciNOCONJ <- function(t,n=100,alpha=0.05,formula='lagsdf~rev+lev'){
neuron=t$neuron[1]
message(paste('Currently working on',neuron))
t %>%
ungroup() %>%
filter(saccade.type != 'conj') %>%
bootstrap(n) %>%
do(tidy(lm(formula,.)))%>%
group_by(term) %>%
summarize(
low=quantile(estimate, alpha / 2),
high=quantile(estimate, 1 - alpha / 2),
m=first(estimate)) ->
ci
ci$neuron<- neuron
return(ci)
}
joinsaccadesuniform<-function(t,buffer=20,threshold=30,saccade.length=150){
findSaccades<-function(ev,threshold=40){
mindur<-50
i<-which(abs(ev)>threshold) #find all the times when speed > threshold
sacoff<-which(diff(i)>mindur) #minimum duration of an accepted saccade
sacon<-c(1,sacoff+1) #first saccade
sacoff<-c(sacoff,length(i)) #end of last saccade
saccade.onset<-i[sacon] #get actual times
saccade.offset<-i[sacoff]
return(data.frame(saccade.onset,saccade.offset))
}
jsac<- function(stimes){
summary(stimes)
#input should be an array of length 2: c(onsettime,offsettime, saccade.number,saccade.dur)
df<- data.frame(time=stimes[[1]]:stimes[[2]])
df$sacnum<- stimes[[4]]
df$saccade.dur<- stimes[[3]]
return(df)
# return(stimes[[1]]:stimes[[2]])
}
stimes<-findSaccades(t$conj.velocity,threshold)
stimes %>%
mutate(dur=saccade.offset-saccade.onset,
s=row_number(),
saccade.onset=saccade.onset-buffer,
###########HERE IS WHERE i MAKE SACCADES UNIFORM #######
saccade.offset=saccade.onset+saccade.length+2*buffer)->
stimes
x<- do.call('rbind',apply(stimes,1,jsac))
x %>%
group_by(sacnum) %>%
mutate(counter=time-first(time)) ->
x
left_join(t,x,by='time')
}
joinsaccades<-function(t,buffer=20,threshold=30){
findSaccades<-function(ev,threshold=40){
mindur<-50
i<-which(abs(ev)>threshold) #find all the times when speed > threshold
sacoff<-which(diff(i)>mindur) #minimum duration of an accepted saccade
sacon<-c(1,sacoff+1) #first saccade
sacoff<-c(sacoff,length(i)) #end of last saccade
saccade.onset<-i[sacon] #get actual times
saccade.offset<-i[sacoff]
return(data.frame(saccade.onset,saccade.offset))
}
jsac<- function(stimes){
#input should be an array of length 2: c(onsettime,offsettime)
df<- data.frame(time=stimes[[1]]:stimes[[2]])
df$sacnum<- stimes[[3]]
return(df)
# return(stimes[[1]]:stimes[[2]])
}
stimes<-findSaccades(t$conj.velocity,threshold)
stimes %>%
mutate(s=row_number(),
saccade.onset=saccade.onset-buffer,
saccade.offset=saccade.offset+buffer)->
stimes
x<- do.call('rbind',apply(stimes,1,jsac))
x %>%
group_by(sacnum) %>%
mutate(counter=time-first(time)) ->
x
left_join(t,x,by='time')
}
markEnhancement<- function(v, threshold1=15,threshold2=8){
require(dplyr)
mindur<-50
i<-which(abs(v)>threshold2) #find all the times when speed is above the lower threshold
sacoff<-which(diff(i)>mindur) #minimum duration of an accepted saccade
sacon<-c(1,sacoff+1) #first saccade
sacoff<-c(sacoff,length(i)) #end of last saccade
event.onset<-i[sacon] #get actual times
event.offset<-i[sacoff]
stimes<- data.frame(event.onset,event.offset)
nsaccades=nrow(stimes)
jsac<- function(stimes){
summary(stimes)
#input should be an array of length 2: c(onsettime,offsettime, saccade.number,saccade.dur)
df<- data.frame(time=stimes[[1]]:stimes[[2]])
df$enhancenum<- stimes[[4]]
df$enhance.dur<- stimes[[3]]
return(df)
# return(stimes[[1]]:stimes[[2]])
}
stimes %>%
mutate(dur=event.offset-event.onset,
s=row_number())->
stimes
x<- do.call('rbind',apply(stimes,1,jsac))
v<- data.frame(v=v)
v<- mutate(v, time=row_number())
xx<- left_join(v,x,by='time')
xx %>%
group_by(enhancenum) %>%
summarize(max.vel=max(abs(v))) %>%
filter(max.vel>threshold1)->
xm
xx %>%
filter(enhancenum %in% unique(xm$enhancenum)) %>%
dplyr::select(time,v,enhancenum) ->
g
}
t<- loadnewcsv(path="C:/Users/setup/Desktop/SOA/",referencefile=NULL)
library(ggplot2)
library(dplyr)
library(knitr)
library(tidyr)
library(broom)
# library(grid)
library(relaimpo)
library(leaps)
#library(data.table)
library(stringr)
# library(checkpoint)
# checkpoint("2017-01-20")
